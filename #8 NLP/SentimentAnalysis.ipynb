{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=[\"The movie was good.\", \n",
    "    \"The movie was not good.\",\n",
    "    \"I really think this product sucks.\",\n",
    "    \"Really great product.\",\n",
    "    \"I don't like this product\"]\n",
    "for t in texts:\n",
    "    print(t, \"==>\", TextBlob(t).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=TextBlob(\"\"\"The movie was good. The movie was not good. I really think this product sucks.\n",
    "Really great product. I don't like this product\"\"\")\n",
    "for s in text.sentences:\n",
    "    print(\"=>\", s)\n",
    "for s in text.sentences:\n",
    "    print(s, \"==> \", s.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating our own classifier\n",
    "Lets use [Sentiment Polarity Dataset 2.0](https://www.cs.cornell.edu/people/pabo/movie-review-data/), included in the `NLTK` library.<Br>\n",
    "It consists of 1000 positive and 1000 negative processed reviews. Introduced in Pang/Lee ACL 2004. Released June 2004."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "\n",
    "from nltk.corpus import movie_reviews as mr\n",
    "print(\"The corpus contains %d reviews\"% len(mr.fileids()))\n",
    "\n",
    "for i in mr.fileids()[995:1005]: # Reviews 995 to 1005\n",
    "    print(i, \"==>\", i.split('/')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the content of one of these reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mr.raw(mr.fileids()[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the frequency of each word in the document ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "FreqDist(mr.raw(mr.fileids()[1]).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the most frequent words in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreq = FreqDist()\n",
    "for i in mr.fileids():\n",
    "    wordfreq += FreqDist(w.lower() for w in mr.raw(i).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous code has flaws because split() is a very basic way of finding the words. Let's use `word_tokenize()` or `mr.words()` instead..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreq = FreqDist()\n",
    "for i in mr.fileids():\n",
    "    wordfreq += FreqDist(w.lower() for w in mr.words(i))\n",
    "print(wordfreq)\n",
    "print(wordfreq.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stop words and punctuation are causing trouble, lets remove them..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopw = stopwords.words('english')\n",
    "wordfreq = FreqDist()\n",
    "for i in mr.fileids():\n",
    "    wordfreq += FreqDist(w.lower() for w in mr.words(i) if w.lower() not in stopw and w.lower() not in string.punctuation)\n",
    "print(wordfreq)\n",
    "print(wordfreq.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets shuffle the documents, otherwise they will remain sorted [\"neg\", \"neg\" ... \"pos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "docnames=mr.fileids()\n",
    "random.shuffle(docnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets split each document into words ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=[]\n",
    "for i in docnames:\n",
    "    y = i.split('/')[0]\n",
    "    documents.append( ( mr.words(i) , y) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our documents..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for docs in documents[:5]:\n",
    "    print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document representation\n",
    "\n",
    "Now, lets produce the final document representation, in the form of a Frequency Distribution ...\n",
    "\n",
    "First, without stop words and punctuation ... (you could use other technique, such as IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopw = stopwords.words('english')\n",
    "docrep=[]\n",
    "for words,tag in documents:\n",
    "    features = FreqDist(w for w in words if w.lower() not in stopw and w.lower() not in string.punctuation)\n",
    "    docrep.append( (features, tag) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our documents again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docrep[:5]:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK classifier: Naive Bayes\n",
    "\n",
    "Defining our training and test sets..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numtrain = int(len(documents) * 80 / 100)  # number of training documents\n",
    "\n",
    "train_set, test_set = docrep[:numtrain], docrep[numtrain:]\n",
    "\n",
    "print(test_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier as nbc\n",
    "\n",
    "classifier = nbc.train(train_set)\n",
    "\n",
    "print(\"Accuracy:\", nltk.classify.accuracy(classifier, test_set))\n",
    "\n",
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
